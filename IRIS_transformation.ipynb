{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1043ec3-02d7-4f92-8b64-8a6af5984550",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Taking the files from web\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ba9ad1-9fec-45fc-b735-a5653214ea61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading the file to create the dataframe\n",
    "df = spark.read.csv(\"file://\"+SparkFiles.get(\"iris.data\"), inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0839fd4e-6a7c-408d-bc68-eab942ff52d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: double (nullable = true)\n |-- _c1: double (nullable = true)\n |-- _c2: double (nullable = true)\n |-- _c3: double (nullable = true)\n |-- _c4: string (nullable = true)\n\n+---+---+---+---+-----------+\n|_c0|_c1|_c2|_c3|        _c4|\n+---+---+---+---+-----------+\n|5.1|3.5|1.4|0.2|Iris-setosa|\n|4.9|3.0|1.4|0.2|Iris-setosa|\n|4.7|3.2|1.3|0.2|Iris-setosa|\n|4.6|3.1|1.5|0.2|Iris-setosa|\n|5.0|3.6|1.4|0.2|Iris-setosa|\n|5.4|3.9|1.7|0.4|Iris-setosa|\n|4.6|3.4|1.4|0.3|Iris-setosa|\n|5.0|3.4|1.5|0.2|Iris-setosa|\n|4.4|2.9|1.4|0.2|Iris-setosa|\n|4.9|3.1|1.5|0.1|Iris-setosa|\n|5.4|3.7|1.5|0.2|Iris-setosa|\n|4.8|3.4|1.6|0.2|Iris-setosa|\n|4.8|3.0|1.4|0.1|Iris-setosa|\n|4.3|3.0|1.1|0.1|Iris-setosa|\n|5.8|4.0|1.2|0.2|Iris-setosa|\n|5.7|4.4|1.5|0.4|Iris-setosa|\n|5.4|3.9|1.3|0.4|Iris-setosa|\n|5.1|3.5|1.4|0.3|Iris-setosa|\n|5.7|3.8|1.7|0.3|Iris-setosa|\n|5.1|3.8|1.5|0.3|Iris-setosa|\n+---+---+---+---+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#priting the column and showing the information\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1135e03d-f1f1-41b5-beda-c05de0f9e84b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+-------------------+\n|_c0|_c1|_c2|_c3|        _c4|            classes|\n+---+---+---+---+-----------+-------------------+\n|5.1|3.5|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.9|3.0|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.7|3.2|1.3|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.6|3.1|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.0|3.6|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.9|1.7|0.4|Iris-setosa|Iris-Dataset-setosa|\n|4.6|3.4|1.4|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.0|3.4|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.4|2.9|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.9|3.1|1.5|0.1|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.7|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.8|3.4|1.6|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.8|3.0|1.4|0.1|Iris-setosa|Iris-Dataset-setosa|\n|4.3|3.0|1.1|0.1|Iris-setosa|Iris-Dataset-setosa|\n|5.8|4.0|1.2|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.7|4.4|1.5|0.4|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.9|1.3|0.4|Iris-setosa|Iris-Dataset-setosa|\n|5.1|3.5|1.4|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.7|3.8|1.7|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.1|3.8|1.5|0.3|Iris-setosa|Iris-Dataset-setosa|\n+---+---+---+---+-----------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df1=df.withColumn(\"classes\", regexp_replace(\"_c4\", \"Iris\", \"Iris-Dataset\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec5f549-8533-4664-9349-706241342df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: ['Iris-Dataset-versicolor', 'Iris-Dataset-virginica', 'Iris-Dataset-setosa']"
     ]
    }
   ],
   "source": [
    "df1.select('classes').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46534249-9994-4f45-bf97-204de7022cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n|summary|               _c0|                _c1|               _c2|               _c3|           _c4|\n+-------+------------------+-------------------+------------------+------------------+--------------+\n|  count|               150|                150|               150|               150|           150|\n|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          null|\n| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          null|\n|    min|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n|    max|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n+-------+------------------+-------------------+------------------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b8edb0-7b4d-4050-9ab5-25d347fef8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-------+\n|_c0|_c1|_c2|_c3|_c4|classes|\n+---+---+---+---+---+-------+\n+---+---+---+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "detail_df=df1.filter(df1['classes'] == \"OH\")\n",
    "detail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282175ec-eda8-4760-919b-b7c68919a493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-------+\n|_c0|_c1|_c2|_c3|_c4|classes|\n+---+---+---+---+---+-------+\n+---+---+---+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#states which starts from H\n",
    "detail_df = df1.filter(df1.classes.startswith(\"s\"))\n",
    "detail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d124609-24a5-486b-9739-9cc97d03cbb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+-------------------+\n|_c0|_c1|_c2|_c3|        _c4|            classes|\n+---+---+---+---+-----------+-------------------+\n|5.1|3.5|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.9|3.0|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.7|3.2|1.3|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.6|3.1|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.0|3.6|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.9|1.7|0.4|Iris-setosa|Iris-Dataset-setosa|\n|4.6|3.4|1.4|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.0|3.4|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.4|2.9|1.4|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.9|3.1|1.5|0.1|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.7|1.5|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.8|3.4|1.6|0.2|Iris-setosa|Iris-Dataset-setosa|\n|4.8|3.0|1.4|0.1|Iris-setosa|Iris-Dataset-setosa|\n|4.3|3.0|1.1|0.1|Iris-setosa|Iris-Dataset-setosa|\n|5.8|4.0|1.2|0.2|Iris-setosa|Iris-Dataset-setosa|\n|5.7|4.4|1.5|0.4|Iris-setosa|Iris-Dataset-setosa|\n|5.4|3.9|1.3|0.4|Iris-setosa|Iris-Dataset-setosa|\n|5.1|3.5|1.4|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.7|3.8|1.7|0.3|Iris-setosa|Iris-Dataset-setosa|\n|5.1|3.8|1.5|0.3|Iris-setosa|Iris-Dataset-setosa|\n+---+---+---+---+-----------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# class which end A\n",
    "detail_df = df1.filter(df1.classes.endswith(\"a\"))\n",
    "detail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af7508a-1a10-4434-9d19-3874ab5d6c57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-------+\n|_c0|_c1|_c2|_c3|_c4|classes|\n+---+---+---+---+---+-------+\n+---+---+---+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "detail_df = df1.filter(df1.classes.like(\"%an%\"))\n",
    "detail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4a8577-e137-416b-896c-3755b59ea3fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "latest_df=spark.createDataFrame(simpleData,schema)\n",
    "latest_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541d6bc9-3640-4ffe-b51c-35a5c016496e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#I want to know the salary of department wise\n",
    "from pyspark.sql.functions import sum\n",
    "salary_by_dept = latest_df.groupBy(\"department\").sum(\"salary\")\n",
    "salary_by_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeecf836-7ece-45ea-8116-fc89778334c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n|department|employee_name|sum(salary)|\n+----------+-------------+-----------+\n|     Sales|      Michael|      86000|\n|     Sales|        James|      90000|\n|   Finance|        Maria|      90000|\n|     Sales|       Robert|      81000|\n|   Finance|        Raman|      99000|\n|   Finance|        Scott|      83000|\n| Marketing|         Jeff|      80000|\n|   Finance|          Jen|      79000|\n| Marketing|        Kumar|      91000|\n+----------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#I want to know Sum of salary department wise ,Department Name, Employe name\n",
    "from pyspark.sql.functions import sum\n",
    "salary_by_dept = latest_df.groupBy(\"department\",\"employee_name\").sum(\"salary\").alias(\"sum_salary\")\n",
    "salary_by_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1856ba-34e6-4e2f-b5f6-188cb6155aba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+----------+\n|employee_name|department|state|salary|age|bonus|Salary Sum|\n+-------------+----------+-----+------+---+-----+----------+\n|        Maria|   Finance|   CA| 90000| 24|23000|    351000|\n|        Raman|   Finance|   CA| 99000| 40|24000|    351000|\n|        Scott|   Finance|   NY| 83000| 36|19000|    351000|\n|          Jen|   Finance|   NY| 79000| 53|15000|    351000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|    171000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|    171000|\n|        James|     Sales|   NY| 90000| 34|10000|    257000|\n|      Michael|     Sales|   NY| 86000| 56|20000|    257000|\n|       Robert|     Sales|   CA| 81000| 30|23000|    257000|\n+-------------+----------+-----+------+---+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window  #Importing the windowfunction\n",
    "windowSpec  = Window.partitionBy(\"Department\") #Partitioning by the department column\n",
    "latest_df.withColumn(\"Salary Sum\",sum(col(\"Salary\")).over(windowSpec)).show() #Creating new column salary sum with sum function using over to calculate the sum on the dataframe with the partitionedÂ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a1db02-0015-4093-89ea-d7c7f5a0add4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IRIS_transformation",
   "notebookOrigID": 2241367733041060,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
